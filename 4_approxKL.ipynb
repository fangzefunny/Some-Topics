{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating KL divergence \n",
    "\n",
    "Reference:\n",
    "\n",
    "http://joschu.net/blog/kl-approx.html\n",
    "\n",
    "https://towardsdatascience.com/approximating-kl-divergence-4151c8c85ddd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief introduction \n",
    "\n",
    "Why do we want an approximation? \n",
    "\n",
    "* The KL divergence may not have an analytical solution. Like the Gaussian mixture distributions. \n",
    "* The integration is computationally expensive. \n",
    "* It is eaiser to cache the old log-probability but not the whole distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate the forward KL divergence \n",
    "\n",
    "The simplest approximation of the KL divergence is to take the mean over all samples.  \n",
    "\n",
    "$$\\sum_x p(x) \\log \\frac{p(x)}{q(x)} \\approx \\frac{1}{N}\\sum_{i=1}^N \\frac{p(x_i)}{q(x_i)}$$\n",
    "\n",
    "When $N=1$, $\\log\\frac{p(x_i)}{q(x_i)} = \\log r$ is the simplest approximation. \n",
    "\n",
    "Next we can apply the important sampling idea,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_x p(x) \\log \\frac{p(x)}{q(x)} = &\\sum_x q(x)\\frac{p(x)}{q(x)} \\log \\frac{p(x)}{q(x)}\\\\\n",
    "=& \\sum_x q(x) r\\log r \\\\\n",
    "\\approx& q(x_i) r_i \\log r_i  \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "However this approximation is of high variance. Also, half of the samples are negative (when $p(x_i) < q(x_i)$), but KL divergence is always non-negative. One way to prevent the negative approximation is to construct a negative correlated construct. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_x p(x) \\log \\frac{p(x)}{q(x)} =& \\sum_x q(x) r\\log r \\\\\n",
    "\\approx& \\sum_x q(x) r\\log r + \\lambda(r-1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is a little bit like adding a constraint $r - 1 \\geq 0$ into a Lagrangian. As we know $\\log x \\leq 1-x$, we can simply choose $\\lambda = 1$ to ensure the approximation is $\\geq 0$, Thus we can write, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_x p(x) \\log \\frac{p(x)}{q(x)} =& \\sum_x q(x) r\\log r \\\\\n",
    "\\approx& \\frac{1}{N}\\sum_{i=1}^N q(x) r_i\\log r_i + \\lambda(r_i-1)\n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "Sometimes, we would sample from $q(x)$, then the approxiamtion would be $r_i\\log r_i + \\lambda(r_i-1)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005000000001000093\n",
      "Bias: -0.00100530397138113, Var: 40.33597307511491\n",
      "Bias: 0.00044284607723210385, Var: 1.4167130974890887\n"
     ]
    }
   ],
   "source": [
    "eps_ =1e-12\n",
    "mu1, mu2, sig = 0, .1, 1\n",
    "p = norm(loc=0,  scale=1)\n",
    "q = norm(loc=.1, scale=1)\n",
    "x = q.rvs(size=(10_000_000,))\n",
    "KL_true = np.log(sig/sig+eps_) + (sig**2 + (mu1-mu2)**2)/(2*sig**2) - 1/2\n",
    "print(KL_true)\n",
    "\n",
    "logr = p.logpdf(x) - q.logpdf(x)\n",
    "r    = np.exp(logr)\n",
    "k    = r*logr + (r-1)\n",
    "k2   = (r-1) - logr\n",
    "\n",
    "print(f'Bias: {(k.mean() - KL_true)/KL_true}, Var: {k.std()/KL_true}')\n",
    "print(f'Bias: {(k2.mean() - KL_true)/KL_true}, Var: {k2.std()/KL_true}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true tensor(0.0050)\n",
      "tensor(0.0127) tensor(20.0023)\n",
      "tensor(-1.1449) tensor(6.9880)\n",
      "tensor(0.0003) tensor(1.4177)\n"
     ]
    }
   ],
   "source": [
    "import torch.distributions as dis\n",
    "p = dis.Normal(loc=0, scale=1)\n",
    "q = dis.Normal(loc=0.1, scale=1)\n",
    "x = q.sample(sample_shape=(10_000_000,))\n",
    "truekl = dis.kl_divergence(p, q)\n",
    "print(\"true\", truekl)\n",
    "logr = p.log_prob(x) - q.log_prob(x)\n",
    "k1 = -logr\n",
    "k2 = (logr.exp()*logr + (logr.exp()-1)) * q.log_prob(x).exp()\n",
    "k3 = (logr.exp() - 1) - logr \n",
    "for k in (k1, k2, k3):\n",
    "    print((k.mean() - truekl) / truekl, k.std() / truekl)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "026a2e0de64d16e4261b338391046de2222e48854e49c02150b0bae443d24681"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
