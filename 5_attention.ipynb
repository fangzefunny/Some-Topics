{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention mechanism\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://arxiv.org/abs/1706.03762\n",
    "\n",
    "https://github.com/sooftware/attentions\n",
    "\n",
    "https://github.com/greentfrapp/attention-primer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import string\n",
    "\n",
    "# visualizatioion pkg \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from utils.viz import viz \n",
    "viz.get_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief introduction\n",
    "\n",
    "The self-attention is one of the most popular and widely used mechanisms in the natural language processing area (NLP). \n",
    "\n",
    "Complex recurrent models are often used to capture the time dependency within the stimulus sequences (for example, phrases, sentences) to ensure the machine reaches a human-level performance in processing and generating languages. Most classic recurrent models with seq2seq training schema (GRU and LSTM) manage the input sequence using a simple philosophy: the input with a longer distance from the current stimulus must have less impact. The training algorithm--backpropagation through time (BPTT)--computes a dramatically lower gradient to the distanced input than the neighborhood. \n",
    "\n",
    "The self-attention mechanism processes the inputs depending on the inputs' representation similarity rather than time distance. The mechanism first estimates the between-input similarity and utilizes this similarity to weight these inputs, according to which it distributes its learning biases (via scaling the gradient). \n",
    "\n",
    "Here comes two implementational-level questions: \n",
    "\n",
    "1. How to calculate the similarity?\n",
    "2. How to combine the computed attention with the input?\n",
    "\n",
    "#### Similarity\n",
    "\n",
    "In 1_RSA, we had introduced a few similarity (distance) metrics. Here, we use the correlation.\n",
    "\n",
    "$$r(X,Y) = \\frac{1}{n-1}\\sum^n_i \\frac{(x_i-\\bar{x})(y_i-\\bar{y})}{s_x s_y}$$ \n",
    "where $\\bar{x}=\\frac{1}{n}\\sum_i^n x_i$, $s_x = \\sqrt{\\frac{1}{n-1}\\sum_i^n (x_i-\\bar{x})^2}$\n",
    "\n",
    "Let's simplify this equation a bit. Assuming the elements in x, y are sampled from a Gaussian distribution $N(0, 1)$, we can remove the mean term $\\bar{x}=0,\\bar{y}=0$ and standard deviation $s_xs_y=1$.The correlation becomes a dot product of two vectors divided by the item number:\n",
    "\n",
    "$$r'(X,Y) = \\frac{1}{n-1}\\sum^n_i x_i y_i = XY^{\\top}$$\n",
    "using the convention $X \\in R^{1\\times N}, X \\in R^{1\\times N}$ \n",
    "\n",
    "When using the attention mechanism, the model input is usually a sequence of vector $S = \\{s^1, s^2, ...\\}$, where the superscript indicates the location of the vector within a sequence. We can conduct a pairwise similarity of this sequence. The \"Attention is all you need\" paper dubbed the first element in each pair as a query, $Q$ and the second element as a key, $K$. The similarities between an arbitrary query and all keys are, \n",
    "\n",
    "$$R(j) = \\frac{Q^jK^{\\top}}{n}$$\n",
    "where $-1$ is always neglected. Because $K \\in N(0,1)$ and each $R(j)$ is approximately an linear combination of $R(j) \\approx \\frac{1}{n}K$, the variance of $R(j)$ is about $\\frac{1}{\\sqrt{n}}$. The similarity is then passed through a softmax function to form an attention distribution. \n",
    "\n",
    "$$A(j) = \\text{softmax}\\left(\\frac{Q^jK^{\\top}}{\\sqrt{n}}\\right)$$\n",
    "\n",
    "Question here: why not $/n$?\n",
    "\n",
    "Repeat the attention calculation for each query, we get a $A$. \n",
    "\n",
    "### Combine the attention and the input\n",
    "\n",
    "First we need to know what is $Q$ and $K$? Both are linear transformation of the input $S$,\n",
    "\n",
    "$$Q = S W_q$$\n",
    "$$K = S W_k$$ \n",
    "where $S\\in R^{N\\times E}$, $N$ is the length of the input sequence, $E$ is the embedding dimension. $W_q, W_k \\in R^{E\\times H}$, h is the hidden layer dimension. $Q, K \\in R^{H\\times E}$, and $A \\in R^{H \\times H} = QK^{\\top}$, \n",
    "\n",
    "Meanwhile, the input multiplies a matrix $V = S W_v, V \\in R^{H \\times V}$ for the simple reason of dimension matching.\n",
    "\n",
    "Attended input is $S' \\in R^{H\\times V}= AV$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example: counting letters\n",
    "\n",
    "The concept of ``attention'' roots in psychology, but the algorithm that implements the attention is not at all psychological. We start with a simple example before discussing a psychology project for better illustration. The simple example is **counting letters**. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a sequence, where each element is a randomly selected letter or null/blank. The task is to count how many times each letter appears in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is of course super simple, but we need it to show the attention we learn. Our hypothesis is: the same letter will share same attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the task \n",
    "class task:\n",
    "\n",
    "    def __init__(self, win_size=10, vocab_size=3):\n",
    "        self.win_size = win_size \n",
    "        self.vocab_size = vocab_size\n",
    "        assert vocab_size <= 26\n",
    "\n",
    "    def next_batch(self, batch_size=100):\n",
    "        # create input seq \n",
    "        seq = np.random.choice(np.arange(self.vocab_size+1), [batch_size, self.win_size])\n",
    "        # one hot encoding \n",
    "        x = np.eye(self.vocab_size+1)[seq]\n",
    "        # create label\n",
    "        lab = x.sum(1)[:, 1:].astype(np.int32)\n",
    "        # one hot encoding\n",
    "        y = np.eye(self.win_size+1)[lab]\n",
    "        return x, y\n",
    "\n",
    "    def toStr(self, samples, labels):\n",
    "        # label\n",
    "        samples = samples.reshape(-1, self.win_size, self.vocab_size+1)\n",
    "        idx  = np.expand_dims(np.argmax(samples, axis=2), 2)\n",
    "        strs = np.array(list(' ' + string.ascii_uppercase))\n",
    "        # label\n",
    "        labels = labels.reshape(-1, self.vocab_size, self.win_size+1)\n",
    "        num  = np.expand_dims(np.argmax(labels, axis=2), 2)\n",
    "        return strs[idx].reshape(-1, self.win_size), num.reshape(-1, self.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input string sequence:\n",
      "[['E' 'C' 'D' ' ' 'A' 'C' 'E' ' ' ' ' 'A']\n",
      " ['D' 'E' 'D' 'A' 'B' 'D' 'E' 'B' 'D' 'C']]\n",
      "\n",
      "Counts of each word:\n",
      "[[2 0 2 1 2]\n",
      " [1 2 1 4 2]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "win_size   = 10 \n",
    "vocab_size = 5\n",
    "np.random.seed(1)\n",
    "\n",
    "# task example \n",
    "t = task(win_size, vocab_size)\n",
    "x, y = t.next_batch(15)\n",
    "\n",
    "# get some samples \n",
    "xx = x[:2, :, :]\n",
    "yy = y[:2, :, :]\n",
    "\n",
    "# visualize samples \n",
    "x_str, y_str = t.toStr(xx,yy) \n",
    "print(f'''\n",
    "Input string sequence:\n",
    "{x_str}\n",
    "\n",
    "Counts of each word:\n",
    "{y_str}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.optim import adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will design our model with attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATTN(nn.Modules):\n",
    "    '''Attention Module\n",
    "\n",
    "    Scaled dot-product attention proposed in \"Attention is all you need\"\n",
    "\n",
    "    Args:\n",
    "        win_size: the size of the sentence window\n",
    "    \n",
    "    Inputs:\n",
    "        Q: (B, H, E), the query array\n",
    "        K: (B, H, E), the key array \n",
    "        V: (B, H, E), the value array \n",
    "\n",
    "    Returns:\n",
    "        context: input weighted by attention \n",
    "        attn: attention \n",
    "\n",
    "    '''\n",
    "    def __init__(self, win_size):\n",
    "        super().__init__()\n",
    "        self.d = np.sqrt(win_size)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # BxHxH = BxHxE b@ BxExH\n",
    "        score = torch.bmm(Q, K.transpose(1, 2)) / self.d \n",
    "        attn  = torch.softmax(score, dim=-1)\n",
    "        # BxHxE = BxHxH b@ BxHxE\n",
    "        context = torch.bmm(attn, V)\n",
    "        return context, attn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7b89e158b719c02a21186c9646700ecf5a8cc5b1b6f738df9b6ffa75e5e74e4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('py3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
