{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention mechanism\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://arxiv.org/abs/1706.03762\n",
    "\n",
    "https://github.com/sooftware/attentions\n",
    "\n",
    "https://github.com/greentfrapp/attention-primer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief introduction\n",
    "\n",
    "The self-attention is one of the most popular and widely used mechanisms in the natural language processing area (NLP). \n",
    "\n",
    "Complex recurrent models are often used to capture the time dependency within the stimulus sequences (for example, phrases, sentences) to ensure the machine reaches a human-level performance in processing and generating languages. Most classic recurrent models with seq2seq training schema (GRU and LSTM) manage the input sequence using a simple philosophy: the input with a longer distance from the current stimulus must have less impact. The training algorithm--backpropagation through time (BPTT)--computes a dramatically lower gradient to the distanced input than the neighborhood. \n",
    "\n",
    "The self-attention mechanism processes the inputs depending on the inputs' representation similarity rather than time distance. The mechanism first estimates the between-input similarity and utilizes this similarity to weight these inputs, according to which it distributes its learning biases (via scaling the gradient). \n",
    "\n",
    "Here comes two implementational-level questions: \n",
    "\n",
    "1. How to calculate the similarity?\n",
    "2. How to combine the computed attention with the input?\n",
    "\n",
    "#### Similarity\n",
    "\n",
    "In 1_RSA, we had introduced a few similarity (distance) metrics. Here, we use the correlation.\n",
    "\n",
    "$$r(X,Y) = \\frac{1}{n-1}\\sum^n_i \\frac{(x_i-\\bar{x})(y_i-\\bar{y})}{s_x s_y}$$ \n",
    "where $\\bar{x}=\\frac{1}{n}\\sum_i^n x_i$, $s_x = \\sqrt{\\frac{1}{n-1}\\sum_i^n (x_i-\\bar{x})^2}$\n",
    "\n",
    "Let's simplify this equation a bit. Assuming the elements in x, y are sampled from a Gaussian distribution $N(0, 1)$, we can remove the mean term $\\bar{x}=0,\\bar{y}=0$ and standard deviation $s_xs_y=1$.The correlation becomes a dot product of two vectors divided by the item number:\n",
    "\n",
    "$$r'(X,Y) = \\frac{1}{n-1}\\sum^n_i x_i y_i = XY^{\\top}$$\n",
    "using the convention $X \\in R^{1\\times N}, X \\in R^{1\\times N}$ \n",
    "\n",
    "When using the attention mechanism, the model input is usually a sequence of vector $S = \\{s^1, s^2, ...\\}$, where the superscript indicates the location of the vector within a sequence. We can conduct a pairwise similarity of this sequence. The \"Attention is all you need\" paper dubbed the first element in each pair as a query, $Q$ and the second element as a key, $K$. The similarities between an arbitrary query and all keys are, \n",
    "\n",
    "$$R(j) = \\frac{Q^jK^{\\top}}{n}$$\n",
    "where $-1$ is always neglected. Because $K \\in N(0,1)$ and each $R(j)$ is approximately an linear combination of $R(j) \\approx \\frac{1}{n}K$, the variance of $R(j)$ is about $\\frac{1}{\\sqrt{n}}$. The similarity is then passed through a softmax function to form an attention distribution. \n",
    "\n",
    "$$A(j) = \\text{softmax}\\left(\\frac{Q^jK^{\\top}}{\\sqrt{n}}\\right)$$\n",
    "\n",
    "Question here: why not $/n$?\n",
    "\n",
    "Repeat the attention calculation for each query, we get a $A$. \n",
    "\n",
    "### Combine the attention and the input\n",
    "\n",
    "First we need to know what is $Q$ and $K$? Both are linear transformation of the input $S$,\n",
    "\n",
    "$$Q = S W_q$$\n",
    "$$K = S W_k$$ \n",
    "where $S\\in R^{N\\times E}$, $N$ is the length of the input sequence, $E$ is the embedding dimension. $W_q, W_k \\in R^{E\\times H}$, h is the hidden layer dimension. $Q, K \\in R^{H\\times E}$, and $A \\in R^{H \\times H} = QK^{\\top}$, \n",
    "\n",
    "Meanwhile, the input multiplies a matrix $V = S W_v, V \\in R^{H \\times V}$ for the simple reason of dimension matching.\n",
    "\n",
    "Attended input is $S' \\in R^{H\\times V}= AV$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "026a2e0de64d16e4261b338391046de2222e48854e49c02150b0bae443d24681"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
